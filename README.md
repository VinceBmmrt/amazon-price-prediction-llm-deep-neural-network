# ðŸ·ï¸ amazon-price-prediction-llm-deep-neural-network

> **English version below** / Version franÃ§aise ci-dessus

---

## ðŸ‡«ðŸ‡· Version FranÃ§aise

### Vue d'ensemble

> *Hybrid ML pipeline combining LLM-based structured summarization with deep neural network regression for price prediction.*

Ce projet implÃ©mente un pipeline ML de bout en bout pour prÃ©dire le prix d'un produit Amazon Ã  partir de sa description textuelle. L'idÃ©e centrale : utiliser un LLM pour structurer et nettoyer la donnÃ©e brute, puis entraÃ®ner un rÃ©seau de neurones profond sur ces rÃ©sumÃ©s â€” de la curation Ã  l'infÃ©rence, sur un dataset de **+820 000 produits**.

**Points forts du pipeline :**
- ðŸ§¹ **Data Cleaning** â€” nettoyage rigoureux sur **+820 000 produits** : filtrage des prix, suppression des SKU, part numbers et champs parasites
- ðŸ¤– **LLM Preprocessing** â€” transformation des descriptions brutes en rÃ©sumÃ©s structurÃ©s via Groq, traitÃ©s en **batches de 1 000 items** en parallÃ¨le
- ðŸ§  **Deep Learning** â€” rÃ©seau de neurones profond de **10 couches**, **4 096 neurones** par couche et **+100 millions de paramÃ¨tres**
- ðŸ“Š **Ã‰valuation statistique** â€” MAE, MSE, RÂ², courbes d'erreur avec intervalles de confiance Ã  95 %
- âš¡ **ScalabilitÃ©** â€” traitement multi-processus (ProcessPool), batch asynchrone JSONL, support CUDA/MPS/CPU

```
pricer/
â”œâ”€â”€ loaders.py            â†’ Chargement parallÃ¨le de +820 000 produits Amazon
â”œâ”€â”€ parser.py             â†’ Nettoyage et filtrage des donnÃ©es brutes
â”œâ”€â”€ items.py              â†’ ModÃ¨le de donnÃ©es structurÃ© (Pydantic)
â”œâ”€â”€ preprocessor.py       â†’ GÃ©nÃ©ration de rÃ©sumÃ©s via LLM (appel unitaire)
â”œâ”€â”€ batch.py              â†’ GÃ©nÃ©ration en masse (batches de 1 000 items)
â”œâ”€â”€ deepneuralnetwork.py  â†’ DNN : 10 couches, 4 096 neurones, blocs rÃ©siduels
â””â”€â”€ evaluator.py          â†’ Ã‰valuation et visualisation des performances
```

---

### ðŸ—ºï¸ Ã‰tapes du projet

Ce projet suit un pipeline ML progressif en 6 Ã©tapes :

| Ã‰tape | Description |
|-------|-------------|
| 1ï¸âƒ£ **Data Curation** | Chargement et nettoyage de +820 000 produits Amazon |
| 2ï¸âƒ£ **Data Pre-processing** | RÃ©sumÃ©s LLM via Groq â€” batches de 1 000 items, jobs asynchrones |
| 3ï¸âƒ£ **Baselines & ML classique** | Random Forest, XGBoost â€” Ã©tablir un score de rÃ©fÃ©rence |
| 4ï¸âƒ£ **Deep Learning & LLMs** | DNN ResNet-style : 10 couches, 4 096 neurones, 5 000 features |
| 5ï¸âƒ£ **Fine-tuning** | Fine-tuning d'un modÃ¨le frontier sur +820 000 exemples |
| 6ï¸âƒ£ **Neural Network + LLM** | Combinaison rÃ©seau de neurones et LLM |

### ðŸ“Š Dataset

Source : **McAuley-Lab/Amazon-Reviews-2023** (HuggingFace)

| Version | Taille | Usage |
|---------|--------|-------|
| ðŸª¶ Lite | ~22 000 produits | DÃ©veloppement & tests rapides |
| ðŸ”¥ Full | ~820 000 produits | EntraÃ®nement complet |

Le dataset est nettoyÃ©, enrichi par LLM, puis poussÃ© sur le **HuggingFace Hub** pour Ãªtre rÃ©utilisÃ© Ã  chaque Ã©tape du pipeline.

---

### âš™ï¸ Installation

```bash
git clone https://github.com/ton-utilisateur/amazon-price-prediction-llm-deep-neural-network.git
cd amazon-price-prediction-llm-deep-neural-network
pip install -r requirements.txt
```

**Variables d'environnement** â€” crÃ©e un fichier `.env` :

```env
GROQ_API_KEY=ta_clÃ©_groq_ici
```

---

### ðŸ“¦ DÃ©pendances principales

| Librairie | Usage |
|-----------|-------|
| `torch` | RÃ©seau de neurones (PyTorch) |
| `pydantic` | Validation du modÃ¨le de donnÃ©es |
| `datasets` | Chargement du dataset HuggingFace |
| `scikit-learn` | Vectorisation & mÃ©triques |
| `groq` / `litellm` | Appels LLM (rÃ©sumÃ©s) |
| `plotly` | Visualisations interactives |
| `tqdm` | Barres de progression |
| `python-dotenv` | Gestion des variables d'environnement |

---

### ðŸ—‚ï¸ Description des modules

#### `pricer/items.py` â€” ModÃ¨le de donnÃ©es
DÃ©finit la classe `Item` via **Pydantic**. Chaque item reprÃ©sente un produit Amazon avec ses champs : `title`, `category`, `price`, `full` (texte brut jusqu'Ã  **4 000 caractÃ¨res**), `summary` (rÃ©sumÃ© LLM), `prompt`, etc.

FonctionnalitÃ©s clÃ©s :
- `make_prompt()` â€” gÃ©nÃ¨re le prompt d'entraÃ®nement
- `test_prompt()` â€” retourne le prompt sans le prix (pour infÃ©rence)
- `push_to_hub()` / `from_hub()` â€” intÃ©gration HuggingFace Hub

#### `pricer/loaders.py` â€” Chargement du dataset
Charge le dataset **McAuley-Lab/Amazon-Reviews-2023** depuis HuggingFace (**+820 000 produits** dans la version complÃ¨te). Face Ã  cette volumÃ©trie, un simple chargement sÃ©quentiel serait prohibitif : le module utilise un `ProcessPoolExecutor` pour distribuer le travail sur tous les cÅ“urs CPU disponibles, en dÃ©coupant le dataset en **chunks de 1 000 Ã©lÃ©ments** traitÃ©s en parallÃ¨le.

#### `pricer/parser.py` â€” Nettoyage des donnÃ©es
CÅ“ur du data engineering. Filtre et nettoie chaque produit :
- Plage de prix acceptÃ©e : **$0.50 â†’ $999.49**
- Supprime les numÃ©ros de piÃ¨ces, SKU alphanumÃ©riques et champs parasites (Best Sellers Rank, numÃ©ros de modÃ¨le, etc.)
- Normalise les poids en livres (supporte : pounds, ounces, grams, milligrams, kilograms)
- Limite le texte Ã  **4 000 caractÃ¨res** max (3 000 par champ)
- Exige un minimum de **600 caractÃ¨res** de contenu â€” les fiches trop courtes sont exclues

> âš ï¸ Un mauvais nettoyage = un modÃ¨le qui apprend du bruit. Cette Ã©tape est critique.

#### `pricer/preprocessor.py` â€” RÃ©sumÃ© LLM (unitaire)
Contient la classe `Preprocessor` qui utilise **litellm** pour appeler le modÃ¨le `groq/openai/gpt-oss-20b` et transformer une description produit longue en rÃ©sumÃ© structurÃ© en **5 champs** :

```
Title / Category / Brand / Description / Details
```

Suit Ã©galement les **tokens consommÃ©s et le coÃ»t total** des appels API (`total_input_tokens`, `total_output_tokens`, `total_cost`) â€” indispensable pour monitorer les dÃ©penses Ã  grande Ã©chelle.

#### `pricer/batch.py` â€” RÃ©sumÃ© LLM (batch scalable)
Version scalable du preprocessor. Traite le dataset entier en **batches de 1 000 items** via des jobs asynchrones Groq :
1. GÃ©nÃ¨re des fichiers `.jsonl` par batch de **1 000 items**
2. Upload les fichiers sur Groq
3. Lance des jobs asynchrones (fenÃªtre de **24h**)
4. RÃ©cupÃ¨re et applique les rÃ©sultats
5. Sauvegarde l'Ã©tat en `.pkl` pour reprendre si interruption

> ðŸ’¡ Sur **820 000 items**, cela reprÃ©sente **820 batches** traitÃ©s de faÃ§on asynchrone â€” beaucoup plus Ã©conomique que de payer 820 000 appels API individuels, et sans risque de timeout.

#### `pricer/deepneuralnetwork.py` â€” ModÃ¨le DNN

Un **DNN (Deep Neural Network)** dÃ©signe un rÃ©seau de neurones comportant de nombreuses couches cachÃ©es â€” par opposition Ã  un rÃ©seau superficiel. Ici, l'architecture pousse la profondeur Ã  l'extrÃªme pour capturer des relations complexes entre les mots d'une description et le prix d'un produit :

| ParamÃ¨tre | Valeur |
|-----------|--------|
| Couches cachÃ©es | **10** |
| Neurones par couche | **4 096** |
| Features d'entrÃ©e | **5 000** (HashingVectorizer) |
| ParamÃ¨tres entraÃ®nables | **~100 millions+** |
| Blocs rÃ©siduels | **8** (skip connections ResNet-style) |
| Batch size | **64** |
| Optimiseur | AdamW (lr=0.001, weight_decay=0.01) |
| Scheduler | CosineAnnealingLR |

```
Input (5 000) â†’ Linear(4 096) â†’ [ResidualBlock x8] â†’ Linear(1) â†’ Prix prÃ©dit
```

Les **blocs rÃ©siduels** (skip connections inspirÃ©es de ResNet) sont la clÃ© pour entraÃ®ner un rÃ©seau aussi profond sans que le gradient ne disparaisse. Les prix sont **transformÃ©s en log-scale** puis standardisÃ©s avant l'entraÃ®nement, ce qui stabilise considÃ©rablement la convergence. Le device est dÃ©tectÃ© automatiquement au lancement : **CUDA > MPS (Apple Silicon) > CPU**.

#### `pricer/evaluator.py` â€” Ã‰valuation
Ã‰value n'importe quelle fonction de prÃ©diction sur un Ã©chantillon (par dÃ©faut **200 points**) en parallÃ¨le via `ThreadPoolExecutor` (**5 workers**) :
- **MÃ©triques** : MAE (erreur absolue moyenne en $), MSE, RÂ²
- **Scatter plot** : prix prÃ©dit vs prix rÃ©el, colorÃ© par prÃ©cision (ðŸŸ¢ erreur < $40 ou < 20%, ðŸŸ¡ < $80 ou < 40%, ðŸ”´ au-delÃ )
- **Courbe d'erreur** : erreur cumulÃ©e avec intervalle de confiance Ã  **95 %**

---

### ðŸš€ Utilisation

```python
# 1. Charger les donnÃ©es (~820 000 produits)
from pricer.loaders import ItemLoader
items = ItemLoader("Electronics").load()

# 2. GÃ©nÃ©rer les rÃ©sumÃ©s en batch (820 batches de 1 000 items)
from pricer.batch import Batch
Batch.create(items, lite=False)
Batch.run()
Batch.fetch()

# 3. EntraÃ®ner le DNN (10 couches, 4 096 neurones, ~100M paramÃ¨tres)
from pricer.deepneuralnetwork import DeepNeuralNetworkRunner
runner = DeepNeuralNetworkRunner(train_items, val_items)
runner.setup()
runner.train(epochs=10)
runner.save("model.pt")

# 4. Ã‰valuer sur 200 points
from pricer.evaluator import evaluate
evaluate(runner.inference, test_items, size=200)
```

---

### ðŸ“Š MÃ©triques d'Ã©valuation

| MÃ©trique | Description |
|----------|-------------|
| **MAE** | Erreur absolue moyenne en dollars |
| **MSE** | Erreur quadratique moyenne |
| **RÂ²** | Coefficient de dÃ©termination (% variance expliquÃ©e) |

---

### ðŸ—ƒï¸ Structure du projet

```
amazon-price-prediction-llm-deep-neural-network/
â””â”€â”€ pricer/
    â”œâ”€â”€ items.py
    â”œâ”€â”€ parser.py
    â”œâ”€â”€ loaders.py
    â”œâ”€â”€ preprocessor.py
    â”œâ”€â”€ batch.py
    â”œâ”€â”€ deepneuralnetwork.py
    â”œâ”€â”€ evaluator.py
    â”œâ”€â”€ .env
    â””â”€â”€ requirements.txt
```

---
---

## ðŸ‡¬ðŸ‡§ English Version

### Overview

> *Hybrid ML pipeline combining LLM-based structured summarization with deep neural network regression for price prediction.*

This project builds an end-to-end ML pipeline to predict an Amazon product's price from its text description. The core idea: use an LLM to structure and clean raw product data, then train a deep neural network on those summaries â€” from curation to inference, across a dataset of **820,000+ products**.

**Pipeline highlights:**
- ðŸ§¹ **Data Cleaning** â€” rigorous cleaning across **820,000+ products**: price filtering, SKU removal, part numbers and junk field stripping
- ðŸ¤– **LLM Preprocessing** â€” transforms raw descriptions into structured summaries via Groq, processed in **batches of 1,000 items** asynchronously
- ðŸ§  **Deep Learning** â€” deep neural network with **10 layers**, **4,096 neurons** per layer and **100M+ trainable parameters**
- ðŸ“Š **Statistical Evaluation** â€” MAE, MSE, RÂ², error curves with 95% confidence intervals
- âš¡ **Scalability** â€” multi-process loading (ProcessPool), async JSONL batch jobs, CUDA/MPS/CPU auto-detection

```
pricer/
â”œâ”€â”€ loaders.py            â†’ Parallel loading of 820,000+ Amazon products
â”œâ”€â”€ parser.py             â†’ Raw data cleaning and filtering
â”œâ”€â”€ items.py              â†’ Structured data model (Pydantic)
â”œâ”€â”€ preprocessor.py       â†’ Single-call LLM summary generation
â”œâ”€â”€ batch.py              â†’ Scalable batch summarization (1,000 items/batch)
â”œâ”€â”€ deepneuralnetwork.py  â†’ DNN: 10 layers, 4,096 neurons, residual blocks
â””â”€â”€ evaluator.py          â†’ Evaluation and visualization
```

---

### ðŸ—ºï¸ Project Roadmap

This project follows a progressive 6-step ML pipeline:

| Step | Description |
|------|-------------|
| 1ï¸âƒ£ **Data Curation** | Loading and cleaning 820,000+ Amazon products |
| 2ï¸âƒ£ **Data Pre-processing** | LLM summaries via Groq â€” batches of 1,000 items, async jobs |
| 3ï¸âƒ£ **Baselines & Classic ML** | Random Forest, XGBoost â€” establish a reference score |
| 4ï¸âƒ£ **Deep Learning & LLMs** | ResNet-style DNN: 10 layers, 4,096 neurons, 5,000 features |
| 5ï¸âƒ£ **Fine-tuning** | Fine-tuning a frontier model on 820,000+ examples |
| 6ï¸âƒ£ **Neural Network + LLM** | Combining neural network and LLM |

### ðŸ“Š Dataset

Source: **McAuley-Lab/Amazon-Reviews-2023** (HuggingFace)

| Version | Size | Usage |
|---------|------|-------|
| ðŸª¶ Lite | ~22,000 products | Development & fast iteration |
| ðŸ”¥ Full | ~820,000 products | Full training |

The dataset is cleaned, LLM-enriched, then pushed to the **HuggingFace Hub** and reused at each stage of the pipeline.

---

### âš™ï¸ Installation

```bash
git clone https://github.com/your-username/amazon-price-prediction-llm-deep-neural-network.git
cd amazon-price-prediction-llm-deep-neural-network
pip install -r requirements.txt
```

**Environment variables** â€” create a `.env` file:

```env
GROQ_API_KEY=your_groq_api_key_here
```

---

### ðŸ“¦ Main Dependencies

| Library | Usage |
|---------|-------|
| `torch` | Neural network (PyTorch) |
| `pydantic` | Data model validation |
| `datasets` | HuggingFace dataset loading |
| `scikit-learn` | Vectorization & metrics |
| `groq` / `litellm` | LLM API calls (summaries) |
| `plotly` | Interactive visualizations |
| `tqdm` | Progress bars |
| `python-dotenv` | Environment variable management |

---

### ðŸ—‚ï¸ Module Descriptions

#### `pricer/items.py` â€” Data Model
Defines the `Item` class using **Pydantic**. Each item represents an Amazon product with fields: `title`, `category`, `price`, `full` (raw text up to **4,000 characters**), `summary` (LLM summary), `prompt`, etc.

Key methods:
- `make_prompt()` â€” generates the training prompt
- `test_prompt()` â€” returns the prompt without the price (for inference)
- `push_to_hub()` / `from_hub()` â€” HuggingFace Hub integration

#### `pricer/loaders.py` â€” Dataset Loading
Loads the **McAuley-Lab/Amazon-Reviews-2023** dataset from HuggingFace (**820,000+ products** in full mode). At this scale, sequential loading would be prohibitive: the module uses a `ProcessPoolExecutor` to distribute work across all available CPU cores, splitting the dataset into **chunks of 1,000 items** processed in parallel.

#### `pricer/parser.py` â€” Data Cleaning
The data engineering core. Filters and cleans each product:
- Accepted price range: **$0.50 â†’ $999.49**
- Removes part numbers, alphanumeric SKUs and noisy fields (Best Sellers Rank, model numbers, etc.)
- Normalizes weights to pounds (supports: pounds, ounces, grams, milligrams, kilograms)
- Caps text at **4,000 characters** max (3,000 per field)
- Requires a minimum of **600 characters** of content â€” short listings are excluded

> âš ï¸ Poor cleaning means the model learns noise instead of signal. This step is critical.

#### `pricer/preprocessor.py` â€” LLM Summarization (single call)
Contains the `Preprocessor` class that uses **litellm** to call the `groq/openai/gpt-oss-20b` model and transform a long product description into a structured **5-field summary**:

```
Title / Category / Brand / Description / Details
```

Also tracks **token usage and total API cost** (`total_input_tokens`, `total_output_tokens`, `total_cost`) â€” essential for monitoring spending at scale.

#### `pricer/batch.py` â€” LLM Summarization (scalable batch)
Scalable version of the preprocessor. Processes the entire dataset in **batches of 1,000 items** via async Groq jobs:
1. Generates `.jsonl` files in batches of **1,000 items**
2. Uploads files to Groq
3. Launches async jobs (completion window: **24h**)
4. Fetches and applies results
5. Saves state as `.pkl` to resume after interruption

> ðŸ’¡ On **820,000 items**, this means **820 batches** processed asynchronously â€” far more cost-effective than paying for 820,000 individual API calls, and with no timeout risk.

#### `pricer/deepneuralnetwork.py` â€” DNN Model

A **DNN (Deep Neural Network)** is a neural network with many hidden layers â€” as opposed to shallow architectures. Here, the depth is pushed to the extreme to capture complex relationships between product description words and price:

| Parameter | Value |
|-----------|-------|
| Hidden layers | **10** |
| Neurons per layer | **4,096** |
| Input features | **5,000** (HashingVectorizer) |
| Trainable parameters | **~100M+** |
| Residual blocks | **8** (ResNet-style skip connections) |
| Batch size | **64** |
| Optimizer | AdamW (lr=0.001, weight_decay=0.01) |
| Scheduler | CosineAnnealingLR |

```
Input (5,000) â†’ Linear(4,096) â†’ [ResidualBlock x8] â†’ Linear(1) â†’ Predicted price
```

**Residual blocks** (ResNet-inspired skip connections) are the key to training a network this deep without gradients vanishing. Prices are **log-transformed** then standardized before training, which significantly stabilizes convergence. The device is auto-detected at startup: **CUDA > MPS (Apple Silicon) > CPU**.

#### `pricer/evaluator.py` â€” Evaluation
Evaluates any prediction function on a data sample (default **200 points**) in parallel via `ThreadPoolExecutor` (**5 workers**):
- **Metrics**: MAE (mean absolute error in $), MSE, RÂ²
- **Scatter plot**: predicted vs actual price, color-coded by accuracy (ðŸŸ¢ error < $40 or < 20%, ðŸŸ¡ < $80 or < 40%, ðŸ”´ beyond)
- **Error curve**: cumulative error with **95% confidence interval**

---

### ðŸš€ Usage

```python
# 1. Load data (~820,000 products)
from pricer.loaders import ItemLoader
items = ItemLoader("Electronics").load()

# 2. Generate summaries in batch (820 batches of 1,000 items)
from pricer.batch import Batch
Batch.create(items, lite=False)
Batch.run()
Batch.fetch()

# 3. Train the DNN (10 layers, 4,096 neurons, ~100M parameters)
from pricer.deepneuralnetwork import DeepNeuralNetworkRunner
runner = DeepNeuralNetworkRunner(train_items, val_items)
runner.setup()
runner.train(epochs=10)
runner.save("model.pt")

# 4. Evaluate on 200 data points
from pricer.evaluator import evaluate
evaluate(runner.inference, test_items, size=200)
```

---

### ðŸ“Š Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **MAE** | Mean Absolute Error in dollars |
| **MSE** | Mean Squared Error |
| **RÂ²** | Coefficient of determination (% variance explained) |

---

### ðŸ—ƒï¸ Project Structure

```
amazon-price-prediction-llm-deep-neural-network/
â””â”€â”€ pricer/
    â”œâ”€â”€ items.py
    â”œâ”€â”€ parser.py
    â”œâ”€â”€ loaders.py
    â”œâ”€â”€ preprocessor.py
    â”œâ”€â”€ batch.py
    â”œâ”€â”€ deepneuralnetwork.py
    â”œâ”€â”€ evaluator.py
    â”œâ”€â”€ .env
    â””â”€â”€ requirements.txt
```

---

### ðŸ“„ License

MIT License â€” feel free to use, modify, and distribute.